#!/usr/bin/env python3
import os
import re
import sys
import json
import asyncio
import subprocess
from subprocess import check_output

from pick import pick

usage = {}
num_running_jobs = None
PERIOD_SEC = 5
MAX_JOBS = 8


MY_UID = os.getuid()


async def main():

    while True:
        build_usage()
        await manage_cgtop_ssh_sessions_until_job_count_changes()
        await asyncio.sleep(1)


def build_usage():
    global usage
    global num_running_jobs
    del usage
    usage = {}
    print("collecting info from slurm...", file=sys.stderr)
    squeue_me = json.loads(check_output("/usr/bin/squeue --me --json", shell=True))
    running_jobs = [x for x in squeue_me["jobs"] if "RUNNING" in x["job_state"]]
    num_running_jobs = len(running_jobs)
    print("done.", file=sys.stderr)
    selected_jobs = select_jobs(running_jobs)
    for job in sorted(selected_jobs, key=lambda x: x["job_id"]):
        jobid = job["job_id"]
        for allocated_node in job["job_resources"]["allocated_nodes"]:
            hostname = allocated_node["nodename"]
            if jobid not in usage:
                usage[jobid] = {}
            if hostname not in usage[jobid]:
                usage[jobid][hostname] = {}
            alloc_cpu_cores = 0
            for socket_data in allocated_node["sockets"].values():
                for core_alloc_type in socket_data["cores"].values():
                    if core_alloc_type == "allocated":
                        alloc_cpu_cores += 1
            # cgroup = f"slurm_{hostname}/uid_{my_uid}/job_{jobid}"
            usage[jobid][hostname] = {
                "pct_cpu_usage": 0,
                "pct_cpu_limit": 100 * alloc_cpu_cores,
                "mem_bytes_usage": 0,
                "mem_bytes_limit": 1000000 * allocated_node["memory_allocated"],
            }


async def manage_cgtop_ssh_sessions_until_job_count_changes():
    if usage == {}:
        clear_terminal_scrollback()
        print("no running jobs.")
    # find exactly 1 job ID for each hostname, doesn't matter which one
    hostname2jobid = {}
    for jobid, host_usage in usage.items():
        for hostname in host_usage.keys():
            hostname2jobid[hostname] = jobid
    tasks = []
    # the cgtop tasks immediately exit when a job ends. Create a task that also immediately exists
    # when a new job starts running.
    tasks.append(return_when_num_running_jobs_is_not_equal_to(num_running_jobs))
    for hostname, jobid in hostname2jobid.items():
        tasks.append(run_cgtop_on_node(jobid, hostname))

    done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
    for task in pending:
        task.cancel()
        try:
            await task
        except asyncio.CancelledError:
            pass


async def return_when_num_running_jobs_is_not_equal_to(x: int, poll_rate_sec=5) -> None:
    while True:
        squeue_out = check_output(
            r"/usr/bin/squeue --me --noheader --states=RUNNING '--format=%i'", shell=True, text=True
        )
        jobids = squeue_out.strip().splitlines()
        if len(jobids) != x:
            return
        await asyncio.sleep(poll_rate_sec)


async def run_cgtop_on_node(jobid, hostname) -> None:
    # with a 1 second period, this would stop after 4 years or so
    # the slurm_{hostname} argument just filters out noise, not required
    cmd = f"/usr/bin/srun '--jobid={jobid}' --overlap systemd-cgtop --raw -n 999999999 'slurm_{hostname}'"
    proc = await asyncio.create_subprocess_shell(
        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE
    )
    async for line in proc.stdout:
        if not line:
            break
        line = line.decode().strip()
        # print(f'line received: "{line}"')
        try:
            cgroup, _, cpu_usage, mem_usage, _, _ = line.strip().split()
        except ValueError:
            # print(
            #     f'systemd-cgtop output "{line}" could not be split into 6 words!', file=sys.stderr
            # )
            continue
        try:
            jobid = int(
                re.fullmatch(rf"slurm_{hostname}/uid_{MY_UID}/job_(\d+)", cgroup).groups(1)[0]
            )
        except AttributeError:
            # print(f'cgroup not relevant: "{cgroup}"', file=sys.stderr)
            continue
        if jobid not in usage:
            # print(f'jobid not relevant: "{jobid}"', file=sys.stderr)
            continue
        usage[jobid][hostname]["pct_cpu_usage"] = process_cgtop_cpu_usage(cpu_usage)
        usage[jobid][hostname]["mem_bytes_usage"] = process_cgtop_mem_usage(mem_usage)
        update_usage_display()
    await proc.wait()


def process_cgtop_cpu_usage(x: str) -> float:
    if x == "-":
        return 0
    return float(x)


def process_cgtop_mem_usage(x: str) -> float:
    if x == "-":
        return 0
    return float(x)


def clear_terminal_scrollback():
    sys.stdout.write("\033c\033[3J")
    sys.stdout.flush()


def quotient_between_0_1(num, den):
    assert not (num > den)
    assert (num >= 0) and (den >= 0)
    if num == 0 and den == 0:
        return 0
    # throws ZeroDivisionError for 0 den
    return num / den


def closest_element_index(_list, target) -> int:
    """
    return the index of the list element which is closest to target
    """
    min_diff = None
    min_diff_index = -1
    for i, element in enumerate(_list):
        diff = element - target
        if i == 0 or abs(diff) < abs(min_diff):
            min_diff = diff
            min_diff_index = i
    return min_diff_index


def generate_progress_bar(frac: float, _len=20) -> str:
    assert frac >= 0 and frac <= 1
    _len -= 2  # subtract beginning and end characters
    num_chars2frac = [x / _len for x in range(_len + 1)]  # [ 0, 1/len, 2/len, ... len/len=1 ]
    num_chars = closest_element_index(
        num_chars2frac, frac
    )  # round `frac` to the nearest character length fraction
    progress_bar = "[" + ("#" * num_chars) + (" " * (_len - num_chars)) + "]"
    return progress_bar


def human_readable(x: int) -> str:
    output = x
    output_suffix = ""
    for suffix in ["K", "M", "G"]:
        if output > 1000:
            output /= 1000
            output_suffix = suffix
    if int(output) == output:
        return f"{int(output)}{output_suffix}"
    return f"{output:.3f}{output_suffix}"


def update_usage_display():
    # print(json.dumps(usage, indent=4))
    clear_terminal_scrollback()
    for jobid, hostname2job_usage in usage.items():
        print(f"job {jobid}:")
        for hostname, job_usage in hostname2job_usage.items():
            print(f"  {hostname}:")
            cpu_usage_frac = min([1, job_usage["pct_cpu_usage"] / job_usage["pct_cpu_limit"]])
            mem_usage_frac = min([1, job_usage["mem_bytes_usage"] / job_usage["mem_bytes_limit"]])
            cpu_progress_bar = generate_progress_bar(cpu_usage_frac, _len=40)
            mem_progress_bar = generate_progress_bar(mem_usage_frac, _len=40)
            cpu_frac_str = (
                f'{(job_usage["pct_cpu_usage"]/100):.2f} / {int(job_usage["pct_cpu_limit"]/100)}'
            )
            mem_frac_str = f'{human_readable(job_usage["mem_bytes_usage"])} / {human_readable(job_usage["mem_bytes_limit"])}'
            print(f"    CPU: {cpu_progress_bar} {cpu_frac_str} cores")
            print(f"    MEM: {mem_progress_bar} {mem_frac_str} bytes")
        print()


def format_jobs(jobs: list) -> list:
    output = []
    for job in jobs:
        output.append(f"{job['job_id']} {job['name']}")
    return output


def select_jobs(jobs: list) -> list:
    selection = pick(
        format_jobs(jobs),
        f"select JobIDs to monitor (max of {MAX_JOBS})",
        multiselect=True,
        min_selection_count=1,
    )
    selected_lines = [x[0] for x in selection]
    selected_jobids = [int(line.split()[0]) for line in selected_lines]
    if len(selected_jobids) > MAX_JOBS:
        print(
            f"too many jobs selected! Please select no more than {MAX_JOBS} jobs.", file=sys.stderr
        )
        return select_jobs(jobs)
    return [x for x in jobs if x["job_id"] in selected_jobids]


if __name__ == "__main__":
    asyncio.run(main())
